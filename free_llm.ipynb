{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW2D/vt90uPn5d4uaHKKAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keeprich/Agentic_AI/blob/main/free_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "DwF8CJtfYuJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lasb9s6JYleQ",
        "outputId": "c893e923-e1f7-49c0-8ae0-6e490c7c431a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
            "  Downloading ollama-0.5.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Downloading ollama-0.5.2-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.6 ollama-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install langchain langchain-ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > /dev/null 2>&1 &"
      ],
      "metadata": {
        "id": "AWOnoJbcYqp1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull tinyllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6fgAH-WZPuY",
        "outputId": "65945f17-3328-41a1-8fd8-8da7a07de248"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community -q\n",
        "!pip install langgraph -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpUnBIALZAxk",
        "outputId": "e7ce9957-35cc-468b-87ae-27c9ef8aa14a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.5/152.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lyAqYyrZMZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nF2uQwxlZMJ_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import Ollama\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Initialize the model\n",
        "llm = Ollama(model=\"tinyllama\")\n",
        "\n",
        "# Define the tool input schemas\n",
        "class AddInput(BaseModel):\n",
        "    a: int = Field(..., description=\"First number\")\n",
        "    b: int = Field(..., description=\"Second number\")\n",
        "\n",
        "@tool(args_schema=AddInput)\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers together.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "class GreetInput(BaseModel):\n",
        "    name: str = Field(..., description=\"Name of the person to greet\")\n",
        "\n",
        "@tool(args_schema=GreetInput)\n",
        "def greet(name: str) -> str:\n",
        "    \"\"\"Return a greeting message for the given name.\"\"\"\n",
        "    return f\"Hello, {name}!\"\n",
        "\n",
        "class WeatherInput(BaseModel):\n",
        "    city: str = Field(..., description=\"City to check the weather for\")\n",
        "\n",
        "@tool(args_schema=WeatherInput)\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Return the current weather in a city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny.\"  # Dummy implementation\n"
      ],
      "metadata": {
        "id": "qwbzogMPYqlr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, greet, get_weather]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "jrLHoBWBYqjY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"What is the weather in Paris?\")\n",
        "print(response)\n",
        "\n",
        "response = agent.run(\"Add 12 and 8.\")\n",
        "print(response)\n",
        "\n",
        "response = agent.run(\"Greet Maria\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gk1_mNuYqhF",
        "outputId": "2a1ec1b0-bc17-4a8b-9950-481ad25eff89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSystem: Respond to the human as helpfully and accurately as possible using a json blob structure and a tool named \"Final Answer\" for final answer. Here's how it works:\n",
            "\n",
            "add: Add two numbers together. Additions are performed by adding the values in both inputs and returning the sum. Args: {'a': {'descripion': 'First number', 'title': 'A', 'type': 'integer'}, 'b': {'descripion': 'Second number', 'title': 'B', 'type': 'integer'}}\n",
            "Greet: Return a greeting message for the given name. Greet: Respond with the current weather in a specific city. Get_weather: Return the current weather in a city. Args: {'city': {'descripion': 'City to check the weather for', 'title': 'City', 'type': 'string'}}\n",
            "\n",
            "Valid \"action\" values: Final Answer or add, greet, get_weather\n",
            "\n",
            "Follow this format for each JSON blob:\n",
            "\n",
            "Question: Input question to answer\n",
            "Thought: Consider previous and subsequent steps. Action: Perform action on inputs using provided tool. \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "System: Respond to the human as helpfully and accurately as possible using a json blob structure and a tool named \"Final Answer\" for final answer. Here's how it works:\n",
            "\n",
            "add: Add two numbers together. Additions are performed by adding the values in both inputs and returning the sum. Args: {'a': {'descripion': 'First number', 'title': 'A', 'type': 'integer'}, 'b': {'descripion': 'Second number', 'title': 'B', 'type': 'integer'}}\n",
            "Greet: Return a greeting message for the given name. Greet: Respond with the current weather in a specific city. Get_weather: Return the current weather in a city. Args: {'city': {'descripion': 'City to check the weather for', 'title': 'City', 'type': 'string'}}\n",
            "\n",
            "Valid \"action\" values: Final Answer or add, greet, get_weather\n",
            "\n",
            "Follow this format for each JSON blob:\n",
            "\n",
            "Question: Input question to answer\n",
            "Thought: Consider previous and subsequent steps. Action: Perform action on inputs using provided tool. \n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSystem: Respond to the human as helpfully and accurately as possible using a JSON blob for actions. Here's how:\n",
            "\n",
            "Add: Add two numbers together.\n",
            "Additional Info: Use this action when the human asks you to add two numbers, such as \"What is the sum of 10 and 25?\"\n",
            "\n",
            "Greet: Return a greeting message for the given name.\n",
            "Additional Info: Use this action when the human asks you for a greeting message for someone's name.\n",
            "\n",
            "Get Weather: Check weather in a city.\n",
            "Additional Info: Use this action when the human wants to know about the current weather in a specific city.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "System: Respond to the human as helpfully and accurately as possible using a JSON blob for actions. Here's how:\n",
            "\n",
            "Add: Add two numbers together.\n",
            "Additional Info: Use this action when the human asks you to add two numbers, such as \"What is the sum of 10 and 25?\"\n",
            "\n",
            "Greet: Return a greeting message for the given name.\n",
            "Additional Info: Use this action when the human asks you for a greeting message for someone's name.\n",
            "\n",
            "Get Weather: Check weather in a city.\n",
            "Additional Info: Use this action when the human wants to know about the current weather in a specific city.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSystem: Respond to the human as helpfully and accurately as possible using the provided $JSON_BLOB for each tool action. Please use the appropriate tool names (i.e., \"Final Answer\" or \"Get Weather\") in the JSON blobs and ensure that they are valid json strings. If a tool requires more than one observation, please provide separate observations for each iteration of the tool action.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "System: Respond to the human as helpfully and accurately as possible using the provided $JSON_BLOB for each tool action. Please use the appropriate tool names (i.e., \"Final Answer\" or \"Get Weather\") in the JSON blobs and ensure that they are valid json strings. If a tool requires more than one observation, please provide separate observations for each iteration of the tool action.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt4all\n",
        "\n",
        "from gpt4all import GPT4All\n",
        "model = GPT4All(\"mistral-7b-instruct-v0.1.Q4_0.gguf\")  # Auto-downloads\n",
        "output = model.generate(\"Tell me about AI\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l27z3TgYYqeZ",
        "outputId": "4130bc97-e121-4501-df79-7bd77c5112aa"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt4all in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2025.7.14)\n",
            " in the field of healthcare.\n",
            "\n",
            "Artificial Intelligence (AI) is rapidly changing the landscape of healthcare by providing new tools and technologies to improve patient outcomes, reduce costs, and enhance operational efficiency. There are several ways that AI is being used in healthcare:\n",
            "\n",
            "1. Medical Imaging Analysis - AI algorithms can analyze medical images such as X-rays, CT scans, and MRIs to detect abnormalities and assist radiologists in making more accurate diagnoses. \n",
            "2. Drug Discovery - AI can be used to accelerate the drug discovery process by predicting how drugs will interact with different biological systems, identifying potential new treatments for diseases, and designing clinical trials.\n",
            "3. Personalized Medicine - AI can help healthcare providers develop personalized treatment plans based on a patient's medical history, genetics, lifestyle, and other factors. This can improve outcomes and reduce costs by avoiding unnecessary tests and treatments. \n",
            "4. Virtual Health Assistants - AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkxqBL4nP-5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llamafile"
      ],
      "metadata": {
        "id": "_GzA-9wtOQMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the llamafile binary\n",
        "!wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.17/llamafile-0.8.17\n",
        "\n",
        "# Download Mistral weights (GGUF format)\n",
        "!wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
        "\n",
        "# Run it\n",
        "!chmod +x llamafile-0.8.17\n",
        "# !./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAuGXo1OP-20",
        "outputId": "0eb996a3-73f0-4b78-8701-cb34196280db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-06 12:09:10--  https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.17/llamafile-0.8.17\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/689773665/8beac7ea-5606-48aa-a946-51a7fe13ba96?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T13%3A02%3A07Z&rscd=attachment%3B+filename%3Dllamafile-0.8.17&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T12%3A01%3A39Z&ske=2025-08-06T13%3A02%3A07Z&sks=b&skv=2018-11-09&sig=IP9sO9v%2BQ5%2F5aS37bMZMD2%2FEuh7USZCR2rl86%2BcWK2U%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDQ4MjQ1MSwibmJmIjoxNzU0NDgyMTUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.yAFl0-DL8x1hW1zmfkL-5XK2kf6IrN-O49cEzC3sdj0&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.8.17&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-06 12:09:11--  https://release-assets.githubusercontent.com/github-production-release-asset/689773665/8beac7ea-5606-48aa-a946-51a7fe13ba96?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-06T13%3A02%3A07Z&rscd=attachment%3B+filename%3Dllamafile-0.8.17&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-06T12%3A01%3A39Z&ske=2025-08-06T13%3A02%3A07Z&sks=b&skv=2018-11-09&sig=IP9sO9v%2BQ5%2F5aS37bMZMD2%2FEuh7USZCR2rl86%2BcWK2U%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDQ4MjQ1MSwibmJmIjoxNzU0NDgyMTUxLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.yAFl0-DL8x1hW1zmfkL-5XK2kf6IrN-O49cEzC3sdj0&response-content-disposition=attachment%3B%20filename%3Dllamafile-0.8.17&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170245990 (162M) [application/octet-stream]\n",
            "Saving to: ‘llamafile-0.8.17’\n",
            "\n",
            "llamafile-0.8.17    100%[===================>] 162.36M   158MB/s    in 1.0s    \n",
            "\n",
            "2025-08-06 12:09:12 (158 MB/s) - ‘llamafile-0.8.17’ saved [170245990/170245990]\n",
            "\n",
            "--2025-08-06 12:09:12--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.105, 3.166.152.44, 3.166.152.65, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1754484963&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDQ4NDk2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AxYTZgg8pjESHnh0w2HcOWvaXN37B%7Ezz%7EMbgQMxmRV2oQcZm-GgOUg%7EEopO964ZT-T8236EpyuzftwwCNVsAGsK2nccdOc2ee0aO2X-ff36BhfiWWucOfT-sZlecKjdjP42dW1W%7EBAYmije3PyBUdlZ3vREQAPvQiiTguf4FljsLZttGwfowrVTtyIokEKsrxpZJC0-U84EUGLI0ng%7E9LSij9zDpeiBoFr8nyx2zskmNaMhpfCaNMMrY0uqx113H3D05tJQV2kuHRySZL9vHP53v1mIxNgaaTwMg-awj4IewKIpVG%7E0aEo4n7c3gzo%7Ec5jhfoKc9li62vHQne3LUJQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-08-06 12:09:12--  https://cdn-lfs.hf.co/repos/46/12/46124cd8d4788fd8e0879883abfc473f247664b987955cc98a08658f7df6b826/14466f9d658bf4a79f96c3f3f22759707c291cac4e62fea625e80c7d32169991?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&Expires=1754484963&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDQ4NDk2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Ni8xMi80NjEyNGNkOGQ0Nzg4ZmQ4ZTA4Nzk4ODNhYmZjNDczZjI0NzY2NGI5ODc5NTVjYzk4YTA4NjU4ZjdkZjZiODI2LzE0NDY2ZjlkNjU4YmY0YTc5Zjk2YzNmM2YyMjc1OTcwN2MyOTFjYWM0ZTYyZmVhNjI1ZTgwYzdkMzIxNjk5OTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=AxYTZgg8pjESHnh0w2HcOWvaXN37B%7Ezz%7EMbgQMxmRV2oQcZm-GgOUg%7EEopO964ZT-T8236EpyuzftwwCNVsAGsK2nccdOc2ee0aO2X-ff36BhfiWWucOfT-sZlecKjdjP42dW1W%7EBAYmije3PyBUdlZ3vREQAPvQiiTguf4FljsLZttGwfowrVTtyIokEKsrxpZJC0-U84EUGLI0ng%7E9LSij9zDpeiBoFr8nyx2zskmNaMhpfCaNMMrY0uqx113H3D05tJQV2kuHRySZL9vHP53v1mIxNgaaTwMg-awj4IewKIpVG%7E0aEo4n7c3gzo%7Ec5jhfoKc9li62vHQne3LUJQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.173.166.116, 18.173.166.94, 18.173.166.89, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.173.166.116|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4368438944 (4.1G) [binary/octet-stream]\n",
            "Saving to: ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’\n",
            "\n",
            "mistral-7b-instruct 100%[===================>]   4.07G   168MB/s    in 42s     \n",
            "\n",
            "2025-08-06 12:09:54 (98.6 MB/s) - ‘mistral-7b-instruct-v0.1.Q4_K_M.gguf’ saved [4368438944/4368438944]\n",
            "\n",
            "\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m      \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m   \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[2m╚\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m╝\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m     \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m     \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[K\u001b[1msoftware\u001b[22m: llamafile 0.8.17\n",
            "\u001b[1mmodel\u001b[22m:    mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "\u001b[1mmode\u001b[22m:     RAW TEXT COMPLETION (base model)\n",
            "\u001b[1mcompute\u001b[22m:             Intel Xeon CPU @ 2.20GHz (broadwell)\n",
            "\u001b[1mserver\u001b[22m:   http://127.0.0.1:8081/\n",
            "\n",
            ">>> i\u001b[J\n",
            "... what is food\u001b[J\n",
            "\u001b[K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3NoLcnDP-pF",
        "outputId": "2b72b50f-4a10-4025-9727-624103bff44f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m      \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m   \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[2m╚\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m╝\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╔\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m  \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m     \u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m║\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[32m█\u001b[39m\u001b[2m╗\u001b[22m\n",
            "\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m     \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m  \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m     \u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\u001b[2m╚\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m═\u001b[22m\u001b[2m╝\u001b[22m\n",
            "\u001b[K\u001b[1msoftware\u001b[22m: llamafile 0.8.17\n",
            "\u001b[1mmodel\u001b[22m:    mistral-7b-instruct-v0.1.Q4_K_M.gguf\n",
            "\u001b[1mmode\u001b[22m:     RAW TEXT COMPLETION (base model)\n",
            "\u001b[1mcompute\u001b[22m:             Intel Xeon CPU @ 2.20GHz (broadwell)\n",
            "\u001b[1mserver\u001b[22m:   http://127.0.0.1:8081/\n",
            "\n",
            "\u001b[K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p \"[INST]Write terraform code to provision ec3 in aws/INST]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ_7TY5TT2TU",
        "outputId": "e0b79972-ca13-4fbc-a021-63dbcca85d67"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "Log start\n",
            "main: build = 1500 (a30b324)\n",
            "main: built with cosmocc (GCC) 11.2.0 for x86_64-linux-cosmo\n",
            "main: seed  = 1754492999\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.16 MiB\n",
            "\u001b[Kllm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 1 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            " [INST]Write terraform code to provision ec3 in aws/INST] To provision an EC2 instance using Terraform in AWS, you can use the following code:\n",
            "```\n",
            "provider \"aws\" {\n",
            "  region = \"us-west-2\"\n",
            "}\n",
            "\n",
            "resource \"aws_instance\" \"example\" {\n",
            "  ami           = \"ami-0c55b159cbfafe1f0\"\n",
            "  instance_type = \"t2.micro\"\n",
            "  key_name      = \"my-key-pair\"\n",
            "\n",
            "  provisioner \"aws_key_pair\" {\n",
            "    key_pair_name = \"my-key-pair\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "In this example, we are provisioning an EC2 instance with the following specifications:\n",
            "\n",
            "* The region where the instance will be located is \"us-west-2\".\n",
            "* The AMI (Amazon Machine Image) to use is \"ami-0c55b159cbfafe1f0\".\n",
            "* The instance type is \"t2.micro\".\n",
            "* The key pair to use is \"my-key-pair\".\n",
            "\n",
            "Note that you will need to have the \"aws\\_key\\_pair\" resource defined in your Terraform configuration, and you will need to have a valid key pair with the specified key name.\n",
            "\n",
            "Once you have defined your Terraform configuration, you can use the `terraform apply` command to provision your EC2 instance in AWS. [end of text]\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =   19018.78 ms\n",
            "llama_print_timings:      sample time =      18.59 ms /   315 runs   (    0.06 ms per token, 16940.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35128.61 ms /    17 tokens ( 2066.39 ms per token,     0.48 tokens per second)\n",
            "llama_print_timings:        eval time =  248934.49 ms /   314 runs   (  792.78 ms per token,     1.26 tokens per second)\n",
            "llama_print_timings:       total time =  284154.33 ms /   331 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1owElkujJws",
        "outputId": "1d76b4ea-4e65-4930-92d7-f4e1b5f4a560"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from typing import List, Optional\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.outputs import Generation\n",
        "\n",
        "\n",
        "class LlamafileLLM(LLM):\n",
        "    \"\"\"\n",
        "    LangChain-compatible wrapper for llamafile binary in Colab.\n",
        "    Assumes llamafile and model are in the current working directory.\n",
        "    \"\"\"\n",
        "\n",
        "    model_path: str = \"./mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "    binary_path: str = \"./llamafile-0.8.17\"\n",
        "    max_tokens: int = 100  # Default max tokens\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        full_prompt = f\"[INST]{prompt}[/INST]\"\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    self.binary_path,\n",
        "                    \"-m\", self.model_path,\n",
        "                    \"-p\", full_prompt,\n",
        "                    \"-n\", str(self.max_tokens),\n",
        "                ],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True\n",
        "            )\n",
        "            output = result.stdout.strip()\n",
        "\n",
        "            # Optional: Stop string truncation (manual, basic)\n",
        "            if stop:\n",
        "                for stop_token in stop:\n",
        "                    if stop_token in output:\n",
        "                        output = output.split(stop_token)[0].strip()\n",
        "\n",
        "            return output\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            raise RuntimeError(f\"Llamafile failed:\\n{e.stderr}\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"llamafile\"\n"
      ],
      "metadata": {
        "id": "THx8lTBeT3cC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x ./llamafile-0.8.17\n"
      ],
      "metadata": {
        "id": "TRV-gTWljRE7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from typing import List, Optional\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.outputs import Generation\n",
        "\n",
        "\n",
        "class LlamafileLLM(LLM):\n",
        "    \"\"\"\n",
        "    LangChain-compatible wrapper for llamafile binary in Colab.\n",
        "    \"\"\"\n",
        "\n",
        "    model_path: str = \"./mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "    binary_path: str = \"./llamafile\"\n",
        "    max_tokens: int = 100  # Number of tokens to generate\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        full_prompt = f\"[INST]{prompt}[/INST]\"\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\n",
        "                    self.binary_path,\n",
        "                    \"-m\", self.model_path,\n",
        "                    \"-p\", full_prompt,\n",
        "                    \"-n\", str(self.max_tokens)\n",
        "                ],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                check=True\n",
        "            )\n",
        "            output = result.stdout.strip()\n",
        "\n",
        "            # Apply stop sequences manually if needed\n",
        "            if stop:\n",
        "                for s in stop:\n",
        "                    if s in output:\n",
        "                        output = output.split(s)[0]\n",
        "\n",
        "            return output\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            raise RuntimeError(f\"llamafile failed:\\n{e.stderr}\")\n",
        "        except FileNotFoundError:\n",
        "            raise RuntimeError(\"llamafile binary or model not found. Check paths.\")\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"llamafile\"\n"
      ],
      "metadata": {
        "id": "_VQNWRjztLZb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_llamafile(prompt):\n",
        "    cmd = [\n",
        "        \"!./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p\", prompt\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    return result.stdout\n",
        "\n",
        "response = run_llamafile(\"[INST]Tell me about llamas[/INST]\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "C-KmvZkUvL3g",
        "outputId": "924686ba-c974-45fb-e3d9-c42709fc0acf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '!./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1974932384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_llamafile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INST]Tell me about llamas[/INST]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1974932384.py\u001b[0m in \u001b[0;36mrun_llamafile\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"!./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     ]\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '!./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "subprocess.run(['!./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p \"[INST]Write terraform code to provision ec3 in aws/INST]\"'], shell=True)"
      ],
      "metadata": {
        "id": "sRMgfxfMvSoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "prompt = '[INST]Write terraform code to provision EC2 in AWS[/INST]'\n",
        "cmd = f'./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p \"{prompt}\"'\n",
        "\n",
        "# subprocess.run(cmd, shell=True)\n",
        "\n",
        "result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n",
        "print(result.stdout)\n",
        "\n",
        "\n",
        "for line in result.stdout:\n",
        "    print(line, end='')  # stream output line by line\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "0LWquWkv9qTR",
        "outputId": "55883c0d-98b4-4bc9-c4e2-4d762d502626"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1744445686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# subprocess.run(cmd, shell=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_llamafile_stream(prompt: str):\n",
        "    formatted_prompt = f'[INST]{prompt}[/INST]'\n",
        "    cmd = f'./llamafile-0.8.17 -m mistral-7b-instruct-v0.1.Q4_K_M.gguf -p \"{formatted_prompt}\"'\n",
        "\n",
        "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')  # stream output live\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "\n",
        "run_llamafile_stream(\"where is uk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGqfCg2aBItx",
        "outputId": "25cdb97e-6e23-4626-94cc-6abf2701b95a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note: if you have an AMD or NVIDIA GPU then you need to pass -ngl 9999 to enable GPU offloading\n",
            "Log start\n",
            "main: build = 1500 (a30b324)\n",
            "main: built with cosmocc (GCC) 11.2.0 for x86_64-linux-cosmo\n",
            "main: seed  = 1754494499\n",
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.16 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 1 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
            "generate: n_ctx = 8192, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "\n",
            " [INST]where is uk[/INST] The United Kingdom (UK) is a country located in Europe. It is composed of four countries: England, Scotland, Wales, and Northern Ireland. The capital city of the UK is London, and the largest city is Birmingham. The UK is known for its rich history, culture, and landmarks, including Stonehenge, Buckingham Palace, and the Tower of London. It is also a member of the European Union and a global superpower, with a strong economy and a diverse population. [end of text]\n",
            "\n",
            "\n",
            "llama_print_timings:        load time =    2277.83 ms\n",
            "llama_print_timings:      sample time =       5.99 ms /   101 runs   (    0.06 ms per token, 16869.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3628.33 ms /    11 tokens (  329.85 ms per token,     3.03 tokens per second)\n",
            "llama_print_timings:        eval time =   73894.28 ms /   100 runs   (  738.94 ms per token,     1.35 tokens per second)\n",
            "llama_print_timings:       total time =   77546.40 ms /   111 tokens\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working"
      ],
      "metadata": {
        "id": "SO2yTAdFvqHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def run_llamafile_stream(prompt: str):\n",
        "    formatted_prompt = f'[INST]{prompt}[/INST]'\n",
        "    cmd = (\n",
        "        './llamafile-0.8.17 '\n",
        "        '-m mistral-7b-instruct-v0.1.Q4_K_M.gguf '\n",
        "        f'-p \"{formatted_prompt}\" '\n",
        "        '--silent-prompt --log-disable -ngl 9999'\n",
        "    )\n",
        "\n",
        "    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')  # prints only the model's response\n",
        "\n",
        "    process.wait()\n"
      ],
      "metadata": {
        "id": "WnPJclAGBO6u"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_llamafile_stream(\"where is uk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q48CvLQzDtLk",
        "outputId": "020c71f4-5fd0-4f80-b1fe-d49c3363b8be"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " UK stands for United Kingdom. It is a country located in Europe and consists of four constituent countries: England, Scotland, Wales, and Northern Ireland. The capital city of the UK is London and its largest city is Birmingham. The UK is a member of the European Union and the United Nations, among other international organizations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llamafile Langchain wraper"
      ],
      "metadata": {
        "id": "DQYZddQyOt6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.language_models.llms import LLM\n",
        "from typing import List, Optional\n",
        "import subprocess\n",
        "\n",
        "class LlamafileLLM(LLM):\n",
        "    llamafile_path: str = \"./llamafile-0.8.17\"\n",
        "    model_path: str = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        formatted_prompt = f\"[INST]{prompt}[/INST]\"\n",
        "        cmd = (\n",
        "            f'{self.llamafile_path} '\n",
        "            f'-m {self.model_path} '\n",
        "            f'-p \"{formatted_prompt}\" '\n",
        "            '--silent-prompt --log-disable -ngl 9999'\n",
        "        )\n",
        "\n",
        "        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)\n",
        "        output_lines = []\n",
        "\n",
        "        for line in process.stdout:\n",
        "            output_lines.append(line)\n",
        "\n",
        "        process.wait()\n",
        "        output = \"\".join(output_lines).strip()\n",
        "\n",
        "        # Optional: apply stop tokens manually\n",
        "        if stop:\n",
        "            for token in stop:\n",
        "                if token in output:\n",
        "                    output = output.split(token)[0]\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"llamafile\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm = LlamafileLLM()\n",
        "print(llm.invoke(\"Tell me about AI\"))\n"
      ],
      "metadata": {
        "id": "cCRoxrRb-tCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamafileLLM()\n",
        "print(llm.invoke(\"Tell me about AI\"))\n"
      ],
      "metadata": {
        "id": "mMRDAhAfGoG2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}